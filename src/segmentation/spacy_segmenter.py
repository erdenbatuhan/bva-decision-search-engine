"""
 File:   spacy_segmenter.py
 Author: Batuhan Erden
"""

import spacy

from src.segmentation.segmenter import Segmenter


class SpacySegmenter(Segmenter):

    def __init__(self, corpus, improved=False):
        super().__init__(corpus=corpus)

        self.improved = improved

        # Load basic English pipeline provided by spacy
        self.nlp = spacy.load("en_core_web_sm")

        if improved:  # Introduce additional exceptions/extensions
            self.extend_language()

    def extend_language(self):
        """
        Introduces additional exceptions/extensions to the language/segmenter

        (1) Handles special legal words:
            'Vet. App.', 'Fed. Cir.', 'Fed. Reg.', 'Pub. L. No.', 'DOCKET NO.', '), DATE))', 'non-Federal', 'CF. 38'
        (2) Handles commas and semicolons after closed parenthesis: (2004), and (2004);
        """

        # Handle special legal words (1) and commas and semicolons after closed parenthesis (2)
        for word in [
            'Vet. App.', 'Fed. Cir.', 'Fed. Reg.', 'Pub. L. No.', 'DOCKET NO.', ')DATE))', 'non-Federal', 'Cf. 38'
        ] + ["), ", "); "]:
            self.nlp.tokenizer.add_special_case(word, [{"ORTH": word}])

    def apply_segmentation(self):
        """
        Generates sentences using Spacy

        :return: Sentences generated by Spacy
        """

        # Generate spacy sentences
        generated_sentences_by_document = {
            document_id: list(self.nlp(self.corpus.annotated_documents_by_id[document_id]["plainText"]).sents)
            for document_id in self.corpus.get_documents_split(self.corpus.train_spans)
        }

        # Analyze segmentation
        self.analyze_segmentation("Spacy Segmentation (%s)" % ("Naive" if not self.improved else "Improved"),
                                  generated_sentences_by_document)

        return generated_sentences_by_document

